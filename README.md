
# ğŸ’¬ Offline ChatGPT Clone with `phi3` + Ollama + Streamlit

A sleek, fully offline chatbot app using Microsoft's **phi3** model and the **Ollama** LLM runtime. Styled like ChatGPT, this app supports:

- âœ… Local chat with LLM (`phi3`)
- âœ… New chat sessions with memory
- âœ… Sidebar for recent chats
- âœ… Light/Dark mode toggle
- âœ… Beautiful Streamlit UI

---

## âš™ï¸ Features

- ğŸ” Offline conversations powered by `phi3` running via Ollama
- ğŸ§  Chat history and conversation tracking
- ğŸ¨ Toggle between Light & Dark themes
- ğŸ—‚ï¸ Recent chat list with auto-titles
- âœ… Fully responsive UI inspired by ChatGPT
- ğŸš€ Easily extensible to other models (Mistral, LLaMA3, etc.)

---

## ğŸ§  Powered By

| Component     | Description                     |
|---------------|----------------------------------|
| `phi3`        | Microsoftâ€™s efficient LLM        |
| `Ollama`      | Lightweight LLM runtime locally  |
| `Streamlit`   | UI framework for Python apps     |

---

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/offline-chatgpt-ollama.git
cd offline-chatgpt-ollama
````

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 3. Install Ollama

Download from: [https://ollama.com/download](https://ollama.com/download)

### 4. Pull the `phi3` Model

```bash
ollama run phi3
```

This command downloads and starts the `phi3` model.

---

## â–¶ï¸ Run the App

```bash
streamlit run app.py
```

App runs locally on:
ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ“¬ Using Postman (Optional)

You can also test the model manually via API:

* **Endpoint:** `POST http://localhost:11434/api/generate`
* **Headers:**

  * `Content-Type: application/json`
* **Body (raw/JSON):**

```json
{
  "model": "phi3",
  "prompt": "What is the capital of France?",
  "stream": false
}
```

### âœ… Expected Response:

```json
{
  "response": "The capital of France is Paris."
}
```

---

## ğŸ“ Project Structure

```
offline-chatgpt-ollama/
â”œâ”€â”€ app.py               # Streamlit app
â”œâ”€â”€ callollama.py        # API call to local phi3 model
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md            # You are here
```

---

## ğŸŒˆ Customization Ideas

* Add voice input/output with `speech_recognition`
* Save chat history to file or database
* Support for other models (`llama3`, `mistral`, `gemma`, etc.)
* Add Markdown support or code highlighting



